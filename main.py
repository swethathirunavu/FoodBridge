import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import folium
from streamlit_folium import st_folium
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, classification_report
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb
import warnings
import datetime
from datetime import timedelta
import hashlib
import time
from scipy import stats
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
import networkx as nx
import joblib
import os

warnings.filterwarnings('ignore')

# Configure page
st.set_page_config(
    page_title="FoodBridge - AI-Powered Food Rescue Analytics",
    page_icon="üç≤",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #2E8B57;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    .prediction-card {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

class FoodBridgeDataScience:
    def __init__(self):
        self.initialize_data()
        self.load_or_train_models()
    
    def initialize_data(self):
        """Generate comprehensive synthetic dataset for food rescue analytics"""
        np.random.seed(42)
        
        # Generate donors data
        self.donors_data = self._generate_donors_data()
        
        # Generate historical donations data
        self.donations_data = self._generate_donations_data()
        
        # Generate volunteer data
        self.volunteers_data = self._generate_volunteers_data()
        
        # Generate weather and external factors
        self.external_factors = self._generate_external_factors()
        
        # Generate network data
        self.network_data = self._generate_network_data()
    
    def _generate_donors_data(self):
        """Generate donor profiles with features for ML"""
        donors = []
        donor_types = ['Restaurant', 'Bakery', 'Grocery Store', 'Catering', 'Hotel', 'Cafe']
        locations = [(12.9716, 77.5946), (13.0827, 80.2707), (11.0168, 76.9558), 
                    (15.2993, 74.1240), (17.3850, 78.4867)]
        
        for i in range(500):
            location = locations[i % len(locations)]
            donor_type = np.random.choice(donor_types)
            
            # Features that affect donation patterns
            donor = {
                'donor_id': f'D_{i:03d}',
                'name': f'{donor_type}_{i}',
                'type': donor_type,
                'latitude': location[0] + np.random.normal(0, 0.1),
                'longitude': location[1] + np.random.normal(0, 0.1),
                'size_score': np.random.uniform(1, 10),  # Business size
                'sustainability_score': np.random.uniform(1, 10),
                'avg_daily_footfall': np.random.randint(50, 1000),
                'years_operating': np.random.randint(1, 20),
                'donation_frequency': np.random.choice(['Daily', 'Weekly', 'Monthly']),
                'preferred_pickup_time': np.random.choice(['Morning', 'Afternoon', 'Evening']),
                'max_donation_capacity': np.random.randint(10, 200)
            }
            donors.append(donor)
        
        return pd.DataFrame(donors)
    
    def _generate_donations_data(self):
        """Generate historical donations with temporal patterns"""
        donations = []
        
        # Generate 2 years of daily data
        start_date = datetime.datetime.now() - timedelta(days=730)
        
        for day in range(730):
            current_date = start_date + timedelta(days=day)
            
            # More donations on weekends and holidays
            day_multiplier = 1.5 if current_date.weekday() >= 5 else 1.0
            
            # Seasonal patterns
            month_multiplier = 1.2 if current_date.month in [11, 12, 1] else 1.0
            
            # Daily donations
            num_donations = int(np.random.poisson(20) * day_multiplier * month_multiplier)
            
            for _ in range(num_donations):
                donor_id = np.random.choice(self.donors_data['donor_id'])
                donor_info = self.donors_data[self.donors_data['donor_id'] == donor_id].iloc[0]
                
                # Food categories with different waste patterns
                food_categories = ['Prepared Food', 'Bakery Items', 'Fruits', 'Vegetables', 
                                 'Dairy', 'Packaged Food', 'Beverages']
                food_type = np.random.choice(food_categories)
                
                # Quantity based on donor size and type
                base_quantity = donor_info['size_score'] * np.random.uniform(0.5, 2)
                quantity_kg = max(1, np.random.normal(base_quantity, base_quantity * 0.3))
                
                # Urgency based on food type
                urgency_map = {'Prepared Food': 4, 'Dairy': 6, 'Bakery Items': 24, 
                              'Fruits': 48, 'Vegetables': 72, 'Packaged Food': 168, 'Beverages': 240}
                hours_to_expire = np.random.normal(urgency_map[food_type], urgency_map[food_type] * 0.3)
                hours_to_expire = max(1, hours_to_expire)
                
                # Success rate based on various factors
                success_probability = min(0.95, 0.5 + 
                                        (donor_info['sustainability_score'] / 20) +
                                        (1 / max(1, hours_to_expire / 24)) * 0.3)
                
                donation = {
                    'donation_id': f'DN_{len(donations):06d}',
                    'donor_id': donor_id,
                    'date': current_date,
                    'food_type': food_type,
                    'quantity_kg': round(quantity_kg, 2),
                    'hours_to_expire': round(hours_to_expire, 1),
                    'pickup_success': np.random.random() < success_probability,
                    'people_fed_estimate': int(quantity_kg * np.random.uniform(2, 4)),
                    'day_of_week': current_date.weekday(),
                    'month': current_date.month,
                    'hour_posted': np.random.randint(6, 22),
                    'weather_condition': np.random.choice(['Sunny', 'Rainy', 'Cloudy']),
                    'latitude': donor_info['latitude'],
                    'longitude': donor_info['longitude']
                }
                donations.append(donation)
        
        return pd.DataFrame(donations)
    
    def _generate_volunteers_data(self):
        """Generate volunteer profiles and activity data"""
        volunteers = []
        
        for i in range(200):
            location = np.random.choice([(12.9716, 77.5946), (13.0827, 80.2707), 
                                       (11.0168, 76.9558), (15.2993, 74.1240)])
            
            volunteer = {
                'volunteer_id': f'V_{i:03d}',
                'name': f'Volunteer_{i}',
                'latitude': location[0] + np.random.normal(0, 0.05),
                'longitude': location[1] + np.random.normal(0, 0.05),
                'experience_months': np.random.randint(1, 60),
                'avg_pickups_per_week': np.random.randint(1, 10),
                'transport_capacity': np.random.choice(['Bike', 'Car', 'Van']),
                'availability_hours': np.random.randint(2, 12),
                'success_rate': np.random.uniform(0.7, 0.98),
                'preferred_food_types': np.random.choice(['All', 'Prepared Food', 'Packaged Food'])
            }
            volunteers.append(volunteer)
        
        return pd.DataFrame(volunteers)
    
    def _generate_external_factors(self):
        """Generate weather and external factors data"""
        factors = []
        start_date = datetime.datetime.now() - timedelta(days=365)
        
        for day in range(365):
            current_date = start_date + timedelta(days=day)
            
            factor = {
                'date': current_date,
                'temperature': np.random.normal(25, 8),
                'rainfall': max(0, np.random.exponential(2)),
                'humidity': np.random.uniform(40, 90),
                'festival_day': np.random.random() < 0.05,  # 5% chance
                'public_holiday': np.random.random() < 0.03,  # 3% chance
                'economic_index': np.random.normal(100, 10)
            }
            factors.append(factor)
        
        return pd.DataFrame(factors)
    
    def _generate_network_data(self):
        """Generate network connections between donors, volunteers, and NGOs"""
        connections = []
        
        for _, donation in self.donations_data.iterrows():
            if donation['pickup_success']:
                volunteer = np.random.choice(self.volunteers_data['volunteer_id'])
                ngo = f'NGO_{np.random.randint(1, 20):02d}'
                
                connection = {
                    'donation_id': donation['donation_id'],
                    'donor_id': donation['donor_id'],
                    'volunteer_id': volunteer,
                    'ngo_id': ngo,
                    'connection_strength': np.random.uniform(0.1, 1.0),
                    'delivery_time_minutes': np.random.normal(45, 15)
                }
                connections.append(connection)
        
        return pd.DataFrame(connections)
    
    def load_or_train_models(self):
        """Load or train ML models"""
        
        # Prepare features for demand prediction
        self.prepare_ml_features()
        
        # Train demand prediction model
        self.demand_model = self.train_demand_prediction_model()
        
        # Train success rate prediction model
        self.success_model = self.train_success_prediction_model()
        
        # Train clustering model for donor segmentation
        self.cluster_model = self.train_clustering_model()
        
        # Train anomaly detection model
        self.anomaly_model = self.train_anomaly_detection_model()
        
        # Train optimization model
        self.optimization_model = self.train_optimization_model()
    
    def prepare_ml_features(self):
        """Prepare features for machine learning"""
        
        # Merge donations with external factors
        self.donations_data['date_only'] = self.donations_data['date'].dt.date
        self.external_factors['date_only'] = self.external_factors['date'].dt.date
        
        self.ml_data = self.donations_data.merge(
            self.external_factors[['date_only', 'temperature', 'rainfall', 'humidity', 'festival_day', 'public_holiday']],
            on='date_only',
            how='left'
        )
        
        # Add donor features
        self.ml_data = self.ml_data.merge(
            self.donors_data[['donor_id', 'size_score', 'sustainability_score', 'avg_daily_footfall', 'type']],
            on='donor_id',
            how='left'
        )
        
        # Feature engineering
        self.ml_data['urgency_category'] = pd.cut(self.ml_data['hours_to_expire'], 
                                                 bins=[0, 6, 24, 72, float('inf')], 
                                                 labels=['Critical', 'High', 'Medium', 'Low'])
        
        self.ml_data['quantity_category'] = pd.cut(self.ml_data['quantity_kg'], 
                                                  bins=5, labels=['XS', 'S', 'M', 'L', 'XL'])
    
    def train_demand_prediction_model(self):
        """Train model to predict food demand"""
        
        # Aggregate daily data
        daily_data = self.ml_data.groupby('date_only').agg({
            'quantity_kg': 'sum',
            'temperature': 'mean',
            'rainfall': 'mean',
            'humidity': 'mean',
            'festival_day': 'first',
            'public_holiday': 'first'
        }).reset_index()
        
        daily_data['day_of_week'] = pd.to_datetime(daily_data['date_only']).dt.dayofweek
        daily_data['month'] = pd.to_datetime(daily_data['date_only']).dt.month
        
        # Prepare features
        features = ['temperature', 'rainfall', 'humidity', 'festival_day', 
                   'public_holiday', 'day_of_week', 'month']
        
        X = daily_data[features]
        y = daily_data['quantity_kg']
        
        # Train model
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X, y)
        
        return model
    
    def train_success_prediction_model(self):
        """Train model to predict pickup success"""
        
        # Prepare features
        le = LabelEncoder()
        features_data = self.ml_data.copy()
        
        # Encode categorical variables
        features_data['food_type_encoded'] = le.fit_transform(features_data['food_type'])
        features_data['type_encoded'] = le.fit_transform(features_data['type'])
        features_data['urgency_encoded'] = features_data['hours_to_expire']
        
        features = ['quantity_kg', 'hours_to_expire', 'size_score', 'sustainability_score',
                   'temperature', 'rainfall', 'humidity', 'day_of_week', 'hour_posted',
                   'food_type_encoded', 'type_encoded']
        
        X = features_data[features].fillna(0)
        y = features_data['pickup_success'].astype(int)
        
        # Train model
        model = xgb.XGBClassifier(random_state=42)
        model.fit(X, y)
        
        return model
    
    def train_clustering_model(self):
        """Train clustering model for donor segmentation"""
        
        # Prepare donor features for clustering
        donor_features = self.donors_data[['size_score', 'sustainability_score', 
                                          'avg_daily_footfall', 'years_operating']].fillna(0)
        
        # Scale features
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(donor_features)
        
        # Train KMeans
        kmeans = KMeans(n_clusters=5, random_state=42)
        clusters = kmeans.fit_predict(scaled_features)
        
        return {'model': kmeans, 'scaler': scaler, 'clusters': clusters}
    
    def train_anomaly_detection_model(self):
        """Train anomaly detection model"""
        
        # Prepare features for anomaly detection
        features = ['quantity_kg', 'hours_to_expire', 'people_fed_estimate', 'day_of_week', 'hour_posted']
        
        X = self.ml_data[features].fillna(0)
        
        # Train Isolation Forest
        model = IsolationForest(contamination=0.1, random_state=42)
        model.fit(X)
        
        return model
    
    def train_optimization_model(self):
        """Train model for route optimization"""
        
        # Simple optimization model for delivery routes
        # This would typically involve more complex algorithms like TSP solvers
        
        return {"status": "trained", "method": "distance_based"}

def main():
    st.markdown('<h1 class="main-header">üç≤ FoodBridge AI Analytics Platform</h1>', unsafe_allow_html=True)
    
    # Initialize the data science system
    if 'foodbridge_ds' not in st.session_state:
        with st.spinner("Initializing AI models and loading data..."):
            st.session_state.foodbridge_ds = FoodBridgeDataScience()
    
    fb_ds = st.session_state.foodbridge_ds
    
    # Sidebar navigation
    st.sidebar.title("üß† AI Analytics Dashboard")
    page = st.sidebar.selectbox(
        "Choose Analysis Type",
        ["üìä Overview & KPIs", "üîÆ Predictive Analytics", "üë• Donor Segmentation", 
         "üó∫Ô∏è Geographic Intelligence", "üìà Time Series Analysis", "üîç Anomaly Detection",
         "üåê Network Analysis", "üéØ Optimization Engine"]
    )
    
    if page == "üìä Overview & KPIs":
        show_overview_dashboard(fb_ds)
    elif page == "üîÆ Predictive Analytics":
        show_predictive_analytics(fb_ds)
    elif page == "üë• Donor Segmentation":
        show_donor_segmentation(fb_ds)
    elif page == "üó∫Ô∏è Geographic Intelligence":
        show_geographic_intelligence(fb_ds)
    elif page == "üìà Time Series Analysis":
        show_time_series_analysis(fb_ds)
    elif page == "üîç Anomaly Detection":
        show_anomaly_detection(fb_ds)
    elif page == "üåê Network Analysis":
        show_network_analysis(fb_ds)
    elif page == "üéØ Optimization Engine":
        show_optimization_engine(fb_ds)

def show_overview_dashboard(fb_ds):
    """Show main KPI dashboard with advanced analytics"""
    
    st.header("üìä Advanced Analytics Overview")
    
    # Key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    total_donations = len(fb_ds.donations_data)
    success_rate = fb_ds.donations_data['pickup_success'].mean()
    total_food_rescued = fb_ds.donations_data['quantity_kg'].sum()
    people_fed = fb_ds.donations_data['people_fed_estimate'].sum()
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <h3>üéØ Total Donations</h3>
            <h2>{total_donations:,}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <h3>‚úÖ Success Rate</h3>
            <h2>{success_rate:.1%}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="metric-card">
            <h3>üçΩÔ∏è Food Rescued</h3>
            <h2>{total_food_rescued:,.0f} kg</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown(f"""
        <div class="metric-card">
            <h3>üë• People Fed</h3>
            <h2>{people_fed:,}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    # Advanced visualizations
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìà Daily Food Rescue Trends")
        daily_trends = fb_ds.donations_data.groupby(fb_ds.donations_data['date'].dt.date)['quantity_kg'].sum().reset_index()
        daily_trends.columns = ['Date', 'Quantity']
        
        fig = px.line(daily_trends, x='Date', y='Quantity', 
                     title="Daily Food Rescue Volume")
        fig.add_scatter(x=daily_trends['Date'], y=daily_trends['Quantity'], 
                       mode='markers', name='Daily Points')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("üéØ Success Rate by Food Type")
        success_by_type = fb_ds.donations_data.groupby('food_type').agg({
            'pickup_success': 'mean',
            'quantity_kg': 'count'
        }).reset_index()
        success_by_type.columns = ['Food Type', 'Success Rate', 'Count']
        
        fig = px.bar(success_by_type, x='Food Type', y='Success Rate',
                    title="Pickup Success Rate by Food Category",
                    color='Success Rate', color_continuous_scale='RdYlGn')
        fig.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
    
    # Correlation heatmap
    st.subheader("üî• Feature Correlation Analysis")
    
    numeric_cols = ['quantity_kg', 'hours_to_expire', 'people_fed_estimate', 
                   'day_of_week', 'hour_posted', 'temperature', 'rainfall', 'humidity']
    
    corr_data = fb_ds.ml_data[numeric_cols].corr()
    
    fig = px.imshow(corr_data, text_auto=True, aspect="auto",
                   title="Feature Correlation Matrix")
    st.plotly_chart(fig, use_container_width=True)

def show_predictive_analytics(fb_ds):
    """Show predictive analytics dashboard"""
    
    st.header("üîÆ AI-Powered Predictive Analytics")
    
    # Demand prediction
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Food Demand Prediction")
        
        # User inputs for prediction
        temp = st.slider("Temperature (¬∞C)", 10, 40, 25)
        rainfall = st.slider("Rainfall (mm)", 0.0, 20.0, 2.0)
        humidity = st.slider("Humidity (%)", 40, 90, 70)
        is_festival = st.checkbox("Festival Day")
        is_holiday = st.checkbox("Public Holiday")
        day_of_week = st.selectbox("Day of Week", 
                                  ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 
                                   'Friday', 'Saturday', 'Sunday'])
        month = st.selectbox("Month", range(1, 13))
        
        # Convert day of week to number
        day_mapping = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,
                      'Friday': 4, 'Saturday': 5, 'Sunday': 6}
        day_num = day_mapping[day_of_week]
        
        # Make prediction
        features = np.array([[temp, rainfall, humidity, is_festival, is_holiday, day_num, month]])
        
        try:
            prediction = fb_ds.demand_model.predict(features)[0]
            
            st.markdown(f"""
            <div class="prediction-card">
                <h3>üéØ Predicted Daily Demand</h3>
                <h2>{prediction:.1f} kg</h2>
            </div>
            """, unsafe_allow_html=True)
            
            # Show confidence intervals
            st.info(f"Expected range: {prediction*0.8:.1f} - {prediction*1.2:.1f} kg")
            
        except Exception as e:
            st.error("Error making prediction. Please check inputs.")
    
    with col2:
        st.subheader("‚úÖ Pickup Success Probability")
        
        # Success prediction inputs
        quantity = st.number_input("Quantity (kg)", 1.0, 100.0, 10.0)
        hours_expire = st.number_input("Hours to Expire", 1.0, 168.0, 24.0)
        donor_size = st.slider("Donor Size Score", 1.0, 10.0, 5.0)
        sustainability = st.slider("Sustainability Score", 1.0, 10.0, 5.0)
        hour_posted = st.slider("Hour Posted", 0, 23, 12)
        
        # Food type and donor type (simplified encoding)
        food_type_map = {'Prepared Food': 0, 'Bakery Items': 1, 'Fruits': 2, 
                        'Vegetables': 3, 'Dairy': 4, 'Packaged Food': 5, 'Beverages': 6}
        food_type = st.selectbox("Food Type", list(food_type_map.keys()))
        
        donor_type_map = {'Restaurant': 0, 'Bakery': 1, 'Grocery Store': 2, 
                         'Catering': 3, 'Hotel': 4, 'Cafe': 5}
        donor_type = st.selectbox("Donor Type", list(donor_type_map.keys()))
        
        # Make success prediction
        success_features = np.array([[quantity, hours_expire, donor_size, sustainability,
                                    temp, rainfall, humidity, day_num, hour_posted,
                                    food_type_map[food_type], donor_type_map[donor_type]]])
        
        try:
            success_prob = fb_ds.success_model.predict_proba(success_features)[0][1]
            
            st.markdown(f"""
            <div class="prediction-card">
                <h3>üìà Success Probability</h3>
                <h2>{success_prob:.1%}</h2>
            </div>
            """, unsafe_allow_html=True)
            
            # Recommendation
            if success_prob > 0.8:
                st.success("üü¢ High success probability - Recommended for posting!")
            elif success_prob > 0.6:
                st.warning("üü° Moderate success probability - Consider optimizing timing or quantity")
            else:
                st.error("üî¥ Low success probability - Review posting strategy")
                
        except Exception as e:
            st.error("Error making success prediction.")
    
    # Feature importance
    st.subheader("üîç Model Feature Importance")
    
    try:
        # Get feature importance from the success model
        feature_names = ['Quantity', 'Hours to Expire', 'Donor Size', 'Sustainability',
                        'Temperature', 'Rainfall', 'Humidity', 'Day of Week', 'Hour Posted',
                        'Food Type', 'Donor Type']
        
        importance_scores = fb_ds.success_model.feature_importances_
        
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': importance_scores
        }).sort_values('Importance', ascending=True)
        
        fig = px.bar(importance_df, x='Importance', y='Feature', orientation='h',
                    title="Feature Importance in Success Prediction Model")
        st.plotly_chart(fig, use_container_width=True)
        
    except Exception as e:
        st.info("Feature importance analysis not available.")

def show_donor_segmentation(fb_ds):
    """Show donor segmentation analysis"""
    
    st.header("üë• AI-Powered Donor Segmentation")
    
    # Cluster analysis
    clusters = fb_ds.cluster_model['clusters']
    cluster_data = fb_ds.donors_data.copy()
    cluster_data['Cluster'] = clusters
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üéØ Donor Clusters")
        
        fig = px.scatter(cluster_data, x='size_score', y='sustainability_score',
                        color='Cluster', hover_data=['type', 'avg_daily_footfall'],
                        title="Donor Segmentation: Size vs Sustainability")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("üìä Cluster Characteristics")
        
        cluster_stats = cluster_data.groupby('Cluster').agg({
            'size_score': 'mean',
            'sustainability_score': 'mean',
            'avg_daily_footfall': 'mean',
            'years_operating': 'mean'
        }).round(2)
        
        st.dataframe(cluster_stats)
    
    # Cluster descriptions
    st.subheader("üè∑Ô∏è Cluster Profiles")
    
    cluster_names = {
        0: "üå± Eco Champions",
        1: "üè¢ Large Corporates", 
        2: "üè™ Local Heroes",
        3: "‚≠ê Premium Partners",
        4: "üöÄ Rising Stars"
    }
    
    for cluster_id in sorted(cluster_data['Cluster'].unique()):
        cluster_subset = cluster_data[cluster_data['Cluster'] == cluster_id]
        
        with st.expander(f"Cluster {cluster_id}: {cluster_names.get(cluster_id, 'Unknown')}"):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Count", len(cluster_subset))
                st.metric("Avg Size Score", f"{cluster_subset['size_score'].mean():.1f}")
            
            with col2:
                st.metric("Sustainability", f"{cluster_subset['sustainability_score'].mean():.1f}")
                st.metric("Avg Footfall", f"{cluster_subset['avg_daily_footfall'].mean():.0f}")
            
            with col3:
                st.metric("Years Operating", f"{cluster_subset['years_operating'].mean():.1f}")
                top_type = cluster_subset['type'].mode().iloc[0] if not cluster_subset['type'].mode().empty else 'Mixed'
                st.metric("Dominant Type", top_type)

def show_geographic_intelligence(fb_ds):
    """Show geographic analysis and intelligence"""
    
    st.header("üó∫Ô∏è Geographic Intelligence & Hotspot Analysis")
    
    # Create base map
    center_lat = fb_ds.donations_data['latitude'].mean()
    center_lon = fb_ds.donations_data['longitude'].mean()
    
    m = folium.Map(location=[center_lat, center_lon], zoom_start=10)
    
    # Add donation heatmap
    from folium.plugins import HeatMap
    
    # Prepare heatmap data
    heat_data = [[row['latitude'], row['longitude'], row['quantity_kg']] 
                 for idx, row in fb_ds.donations_data.iterrows()]
    
    HeatMap(heat_data).add_to(m)
    
    # Add cluster markers for donors
    colors = ['red', 'blue', 'green', 'purple', 'orange']
    clusters = fb_ds.cluster_model['clusters']
    
    for idx, row in fb_ds.donors_data.iterrows():
        cluster_color = colors[clusters[idx] % len(colors)]
        
        folium.CircleMarker(
            location=[row['latitude'], row['longitude']],
            radius=5,
            popup=f"Type: {row['type']}<br>Size: {row['size_score']}<br>Cluster: {clusters[idx]}",
            color=cluster_color,
            fillColor=cluster_color,
            fillOpacity=0.6
        ).add_to(m)
    
    # Display map
    st.subheader("üå°Ô∏è Food Waste Heat Map & Donor Clusters")
    st_folium(m, width=700, height=500)
    
    # Geographic statistics
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìç Geographic Distribution Analysis")
        
        # Density analysis by area
        fb_ds.donations_data['lat_rounded'] = fb_ds.donations_data['latitude'].round(2)
        fb_ds.donations_data['lon_rounded'] = fb_ds.donations_data['longitude'].round(2)
        
        density_analysis = fb_ds.donations_data.groupby(['lat_rounded', 'lon_rounded']).agg({
            'quantity_kg': 'sum',
            'pickup_success': 'mean',
            'donation_id': 'count'
        }).reset_index()
        density_analysis.columns = ['Latitude', 'Longitude', 'Total_Kg', 'Success_Rate', 'Count']
        
        # Top hotspots
        top_hotspots = density_analysis.nlargest(10, 'Total_Kg')
        st.dataframe(top_hotspots)
    
    with col2:
        st.subheader("üéØ Success Rate by Location")
        
        fig = px.scatter(density_analysis, x='Total_Kg', y='Success_Rate', 
                        size='Count', color='Success_Rate',
                        title="Food Volume vs Success Rate by Location",
                        color_continuous_scale='RdYlGn')
        st.plotly_chart(fig, use_container_width=True)

def show_time_series_analysis(fb_ds):
    """Show time series analysis and forecasting"""
    
    st.header("üìà Advanced Time Series Analysis & Forecasting")
    
    # Prepare time series data
    ts_data = fb_ds.donations_data.groupby(fb_ds.donations_data['date'].dt.date).agg({
        'quantity_kg': 'sum',
        'pickup_success': 'mean',
        'donation_id': 'count'
    }).reset_index()
    ts_data.columns = ['Date', 'Total_Quantity', 'Success_Rate', 'Count']
    ts_data['Date'] = pd.to_datetime(ts_data['Date'])
    ts_data = ts_data.sort_values('Date')
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Historical Trends")
        
        fig = make_subplots(rows=3, cols=1, 
                           subplot_titles=['Daily Food Volume', 'Success Rate', 'Number of Donations'],
                           vertical_spacing=0.1)
        
        fig.add_trace(go.Scatter(x=ts_data['Date'], y=ts_data['Total_Quantity'],
                                name='Food Volume (kg)', line=dict(color='blue')), row=1, col=1)
        
        fig.add_trace(go.Scatter(x=ts_data['Date'], y=ts_data['Success_Rate'],
                                name='Success Rate', line=dict(color='green')), row=2, col=1)
        
        fig.add_trace(go.Scatter(x=ts_data['Date'], y=ts_data['Count'],
                                name='Donation Count', line=dict(color='red')), row=3, col=1)
        
        fig.update_layout(height=600, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("üîÆ Forecasting & Seasonality")
        
        # Simple moving averages and trends
        ts_data['MA_7'] = ts_data['Total_Quantity'].rolling(window=7).mean()
        ts_data['MA_30'] = ts_data['Total_Quantity'].rolling(window=30).mean()
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=ts_data['Date'], y=ts_data['Total_Quantity'],
                                mode='lines', name='Daily Volume', opacity=0.6))
        fig.add_trace(go.Scatter(x=ts_data['Date'], y=ts_data['MA_7'],
                                mode='lines', name='7-Day MA', line=dict(width=2)))
        fig.add_trace(go.Scatter(x=ts_data['Date'], y=ts_data['MA_30'],
                                mode='lines', name='30-Day MA', line=dict(width=2)))
        
        fig.update_layout(title="Food Volume with Moving Averages", height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        # Weekly patterns
        ts_data['DayOfWeek'] = ts_data['Date'].dt.day_name()
        weekly_pattern = ts_data.groupby('DayOfWeek')['Total_Quantity'].mean().reindex(
            ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        )
        
        fig = px.bar(x=weekly_pattern.index, y=weekly_pattern.values,
                    title="Average Food Volume by Day of Week")
        st.plotly_chart(fig, use_container_width=True)
    
    # Seasonal decomposition
    st.subheader("üîÑ Seasonal Decomposition Analysis")
    
    try:
        # Set date as index for decomposition
        ts_for_decomp = ts_data.set_index('Date')['Total_Quantity'].fillna(method='ffill')
        
        if len(ts_for_decomp) >= 14:  # Need at least 2 periods for decomposition
            decomposition = seasonal_decompose(ts_for_decomp, model='additive', period=7)
            
            fig = make_subplots(rows=4, cols=1, 
                               subplot_titles=['Original', 'Trend', 'Seasonal', 'Residual'])
            
            fig.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed.values,
                                    name='Original'), row=1, col=1)
            fig.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend.values,
                                    name='Trend'), row=2, col=1)
            fig.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal.values,
                                    name='Seasonal'), row=3, col=1)
            fig.add_trace(go.Scatter(x=decomposition.resid.index, y=decomposition.resid.values,
                                    name='Residual'), row=4, col=1)
            
            fig.update_layout(height=800, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Need more historical data for seasonal decomposition analysis.")
    
    except Exception as e:
        st.warning("Seasonal decomposition analysis not available with current data.")

def show_anomaly_detection(fb_ds):
    """Show anomaly detection analysis"""
    
    st.header("üîç AI-Powered Anomaly Detection")
    
    # Detect anomalies
    features = ['quantity_kg', 'hours_to_expire', 'people_fed_estimate', 'day_of_week', 'hour_posted']
    X = fb_ds.ml_data[features].fillna(0)
    
    anomaly_scores = fb_ds.anomaly_model.decision_function(X)
    anomalies = fb_ds.anomaly_model.predict(X)
    
    # Add results to dataframe
    fb_ds.donations_data['anomaly_score'] = anomaly_scores
    fb_ds.donations_data['is_anomaly'] = anomalies == -1
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üö® Anomaly Detection Results")
        
        anomaly_count = fb_ds.donations_data['is_anomaly'].sum()
        anomaly_rate = anomaly_count / len(fb_ds.donations_data) * 100
        
        st.metric("Anomalies Detected", f"{anomaly_count}")
        st.metric("Anomaly Rate", f"{anomaly_rate:.2f}%")
        
        # Show most anomalous donations
        st.subheader("üîç Most Unusual Donations")
        anomalous_donations = fb_ds.donations_data[fb_ds.donations_data['is_anomaly']].nsmallest(10, 'anomaly_score')
        
        display_cols = ['donor_id', 'food_type', 'quantity_kg', 'hours_to_expire', 'pickup_success']
        st.dataframe(anomalous_donations[display_cols])
    
    with col2:
        st.subheader("üìä Anomaly Score Distribution")
        
        fig = px.histogram(fb_ds.donations_data, x='anomaly_score', 
                          color='is_anomaly', nbins=50,
                          title="Distribution of Anomaly Scores")
        st.plotly_chart(fig, use_container_width=True)
        
        # 2D anomaly visualization
        fig = px.scatter(fb_ds.donations_data, x='quantity_kg', y='hours_to_expire',
                        color='is_anomaly', title="Anomalies in Quantity vs Expiry Time")
        st.plotly_chart(fig, use_container_width=True)
    
    # Anomaly patterns analysis
    st.subheader("üìà Anomaly Patterns Analysis")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Anomalies by food type
        anomaly_by_type = fb_ds.donations_data.groupby('food_type')['is_anomaly'].agg(['count', 'sum']).reset_index()
        anomaly_by_type['anomaly_rate'] = anomaly_by_type['sum'] / anomaly_by_type['count']
        
        fig = px.bar(anomaly_by_type, x='food_type', y='anomaly_rate',
                    title="Anomaly Rate by Food Type")
        fig.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Anomalies by hour
        anomaly_by_hour = fb_ds.donations_data.groupby('hour_posted')['is_anomaly'].agg(['count', 'sum']).reset_index()
        anomaly_by_hour['anomaly_rate'] = anomaly_by_hour['sum'] / anomaly_by_hour['count']
        
        fig = px.line(anomaly_by_hour, x='hour_posted', y='anomaly_rate',
                     title="Anomaly Rate by Hour Posted")
        st.plotly_chart(fig, use_container_width=True)
    
    with col3:
        # Anomalies by success
        success_anomaly = fb_ds.donations_data.groupby('pickup_success')['is_anomaly'].mean()
        
        fig = px.bar(x=['Failed', 'Successful'], y=[success_anomaly[False], success_anomaly[True]],
                    title="Anomaly Rate: Failed vs Successful Pickups")
        st.plotly_chart(fig, use_container_width=True)

def show_network_analysis(fb_ds):
    """Show network analysis of donors, volunteers, and NGOs"""
    
    st.header("üåê Social Network Analysis")
    
    # Build network graph
    G = nx.Graph()
    
    # Add nodes
    for donor in fb_ds.donors_data['donor_id']:
        G.add_node(donor, type='donor')
    
    for volunteer in fb_ds.volunteers_data['volunteer_id']:
        G.add_node(volunteer, type='volunteer')
    
    # Add edges based on successful connections
    for _, connection in fb_ds.network_data.iterrows():
        G.add_edge(connection['donor_id'], connection['volunteer_id'], 
                  weight=connection['connection_strength'])
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Network Statistics")
        
        st.metric("Total Nodes", G.number_of_nodes())
        st.metric("Total Connections", G.number_of_edges())
        
        if G.number_of_edges() > 0:
            st.metric("Network Density", f"{nx.density(G):.4f}")
            
            # Centrality measures
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(G)
            
            # Top connected nodes
            top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
            top_between = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
            
            st.subheader("üîó Most Connected Nodes")
            for node, centrality in top_degree:
                node_type = 'Donor' if node.startswith('D_') else 'Volunteer'
                st.write(f"{node_type}: {node} - Centrality: {centrality:.3f}")
            
            st.subheader("üåâ Bridge Nodes (High Betweenness)")
            for node, centrality in top_between:
                if centrality > 0:
                    node_type = 'Donor' if node.startswith('D_') else 'Volunteer'
                    st.write(f"{node_type}: {node} - Betweenness: {centrality:.3f}")
    
    with col2:
        st.subheader("üìà Network Metrics Visualization")
        
        # Degree distribution
        degrees = [G.degree(n) for n in G.nodes()]
        
        fig = px.histogram(x=degrees, nbins=20, title="Degree Distribution")
        st.plotly_chart(fig, use_container_width=True)
        
        # Connection strength analysis
        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
        
        fig = px.histogram(x=edge_weights, nbins=20, title="Connection Strength Distribution")
        st.plotly_chart(fig, use_container_width=True)
    
    # Community detection
    st.subheader("üë• Community Detection")
    
    try:
        communities = nx.community.greedy_modularity_communities(G)
        
        st.write(f"Found {len(communities)} communities in the network")
        
        # Show community sizes
        community_sizes = [len(community) for community in communities]
        
        fig = px.bar(x=range(len(community_sizes)), y=community_sizes,
                    title="Community Sizes")
        st.plotly_chart(fig, use_container_width=True)
        
        # Community details
        for i, community in enumerate(communities[:5]):  # Show top 5 communities
            donors_in_community = [node for node in community if node.startswith('D_')]
            volunteers_in_community = [node for node in community if node.startswith('V_')]
            
            st.write(f"**Community {i+1}:** {len(donors_in_community)} donors, {len(volunteers_in_community)} volunteers")
    
    except Exception as e:
        st.info("Community detection not available for current network structure.")

def show_optimization_engine(fb_ds):
    """Show optimization recommendations"""
    
    st.header("üéØ AI-Powered Optimization Engine")
    
    # Route optimization
    st.subheader("üöö Delivery Route Optimization")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìç Route Planning")
        
        # Select active donations for route optimization
        active_donations = fb_ds.donations_data.head(20)  # Simulate active donations
        
        # Simple route optimization simulation
        route_efficiency = np.random.uniform(0.6, 0.9, len(active_donations))
        
        optimization_results = pd.DataFrame({
            'Donation_ID': active_donations['donation_id'],
            'Location': active_donations[['latitude', 'longitude']].apply(
                lambda x: f"({x['latitude']:.3f}, {x['longitude']:.3f})", axis=1),
            'Urgency_Hours': active_donations['hours_to_expire'],
            'Quantity_kg': active_donations['quantity_kg'],
            'Route_Efficiency': route_efficiency,
            'Priority_Score': active_donations['hours_to_expire'].max() - active_donations['hours_to_expire'] + route_efficiency
        })
        
        # Sort by priority score
        optimization_results = optimization_results.sort_values('Priority_Score', ascending=False)
        
        st.dataframe(optimization_results)
    
    with col2:
        st.subheader("‚è∞ Optimal Timing Analysis")
        
        # Analyze best posting times
        hourly_success = fb_ds.donations_data.groupby('hour_posted')['pickup_success'].mean()
        
        fig = px.bar(x=hourly_success.index, y=hourly_success.values,
                    title="Success Rate by Posting Hour")
        st.plotly_chart(fig, use_container_width=True)
        
        # Day of week optimization
        daily_success = fb_ds.donations_data.groupby('day_of_week')['pickup_success'].mean()
        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        
        fig = px.bar(x=day_names, y=daily_success.values,
                    title="Success Rate by Day of Week")
        st.plotly_chart(fig, use_container_width=True)
    
    # Optimization recommendations
    st.subheader("üí° AI Recommendations")
    
    recommendations = [
        {
            "Type": "‚è∞ Timing",
            "Recommendation": f"Best posting time: {hourly_success.idxmax()}:00",
            "Impact": "Up to 15% increase in pickup success",
            "Confidence": "High"
        },
        {
            "Type": "üìç Location",
            "Recommendation": "Focus on high-density areas during peak hours",
            "Impact": "20% reduction in delivery time",
            "Confidence": "Medium"
        },
        {
            "Type": "üçΩÔ∏è Food Type",
            "Recommendation": "Prioritize prepared food donations (highest urgency)",
            "Impact": "25% reduction in food waste",
            "Confidence": "High"
        },
        {
            "Type": "üë• Volunteer",
            "Recommendation": "Deploy volunteers based on historical success patterns",
            "Impact": "30% improvement in coverage",
            "Confidence": "Medium"
        }
    ]
    
    for rec in recommendations:
        with st.expander(f"{rec['Type']}: {rec['Recommendation']}"):
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**Expected Impact:** {rec['Impact']}")
            with col2:
                st.write(f"**Confidence Level:** {rec['Confidence']}")
    
    # Resource allocation optimization
    st.subheader("üìä Resource Allocation Optimization")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Volunteer allocation
        volunteer_workload = fb_ds.network_data.groupby('volunteer_id').size().reset_index()
        volunteer_workload.columns = ['Volunteer_ID', 'Current_Workload']
        
        # Simulate optimal workload
        volunteer_workload['Optimal_Workload'] = np.random.poisson(
            volunteer_workload['Current_Workload'].mean(), 
            len(volunteer_workload)
        )
        volunteer_workload['Efficiency_Score'] = (
            volunteer_workload['Optimal_Workload'] / 
            (volunteer_workload['Current_Workload'] + 1)
        )
        
        fig = px.scatter(volunteer_workload, x='Current_Workload', y='Optimal_Workload',
                        color='Efficiency_Score', title="Volunteer Workload Optimization")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Coverage gap analysis
        coverage_data = pd.DataFrame({
            'Area': ['North', 'South', 'East', 'West', 'Central'],
            'Current_Coverage': [0.8, 0.6, 0.9, 0.5, 0.95],
            'Demand_Level': [0.7, 0.9, 0.6, 0.8, 0.85],
            'Gap': [0.1, -0.3, 0.3, -0.3, 0.1]
        })
        
        fig = px.bar(coverage_data, x='Area', y='Gap',
                    color='Gap', color_continuous_scale='RdYlGn',
                    title="Coverage Gap Analysis by Area")
        st.plotly_chart(fig, use_container_width=True)

if __name__ == "__main__":
    main()
